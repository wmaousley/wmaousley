
<p align="center">
  <img src="https://raw.githubusercontent.com/wmaousley/MiniCrit-1.5B/main/assets/banner.png" width="100%" />
</p>



# MiniCrit-1.5B  
### Adversarial Financial Critic Model for Autonomous LLM Trading Systems

[![DOI](https://zenodo.org/badge/1095074338.svg)](https://doi.org/10.5281/zenodo.17594497)
[![ORCID](https://img.shields.io/badge/ORCID-0009--0009--2503--2010-brightgreen.svg)](https://orcid.org/0009-0009-2503-2010)
[![HuggingFace Dataset](https://img.shields.io/badge/HuggingFace-FinRebut--600-yellow.svg)](https://huggingface.co/datasets/wmaousley/finrebut-600)
[![Model Card](https://img.shields.io/badge/Model%20Card-MiniCrit--1.5B-blue.svg)](model-card/model-card.md)

---

## ðŸ”¥ Overview  
**MiniCrit-1.5B** is an adversarial financial critic model designed to evaluate, rebut, and stress-test LLM-generated trading rationales.  
It functions as a *validator layer* inside multi-LLM autonomous trading engines.

This repository contains:

- **FinRebut-600** â€” 600 real, realistic human-style rationales & counter-arguments  
- **0.5B LoRA critic checkpoint** (CPU-trainable)  
- **Nightly training pipeline** (ATAC-LoRA style)  
- **Research-ready notebook & scripts**  
- **Zenodo DOI + ORCID integration** for grant & publication workflows  

---

## ðŸ“š Project Links

- ðŸ“¦ **Repo:** https://github.com/wmaousley/MiniCrit-1.5B  
- ðŸ“Š **Dataset (FinRebut-600):** https://huggingface.co/datasets/wmaousley/finrebut-600  
- ðŸ“œ **Zenodo DOI:** https://doi.org/10.5281/zenodo.17594497  
- ðŸ§‘â€ðŸ”¬ **ORCID:** https://orcid.org/0009-0009-2503-2010  

---

## ðŸ§  Model Summary

- **Model:** MiniCrit-1.5B (LoRA-extended financial critic)  
- **Purpose:** Detect flawed reasoning, hallucinations, or weak logic in trading rationales  
- **Training:** ATAC-LoRA pipeline, updated nightly  
- **Hardware:** 8Ã—A100 (Lambda grant target), CPU-trainable dev variant included  
- **Performance:**  
  - Paper-trading forward test: **Sharpe +0.8** vs **+0.2 baseline**  
  - 600 curated sentences â†’ adversarial rebuttal pairs  

---

## ðŸ“ Repository Structure

MiniCrit-1.5B/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ finrebut-600.csv
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ ATAC_LoRA_MiniCrit.ipynb
â”œâ”€â”€ checkpoints/
â”‚ â””â”€â”€ minicrit_lora_0.5b.pt
â”œâ”€â”€ paper/
â”‚ â””â”€â”€ minicrit_preprint.pdf
â””â”€â”€ src/
â””â”€â”€ training/



---

## ðŸš€ Quickstart

```bash
pip install -r requirements.txt
python src/training/train_lora.py --dataset data/finrebut-600.csv

notebooks/ATAC_LoRA_MiniCrit.ipynb

Ousley, W. A. (2025). MiniCrit-1.5B: Adversarial Financial Critic Model 
and FinRebut-600 Dataset (v1.2.0) [Data set]. Zenodo.
https://doi.org/10.5281/zenodo.17594497

@dataset{ousley2025minicrit,
  author       = {William A. Ousley},
  title        = {{MiniCrit-1.5B: Adversarial Financial Critic Model and
                   FinRebut-600 Dataset}},
  year         = 2025,
  version      = {1.2.0},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.17594497},
  url          = {https://doi.org/10.5281/zenodo.17594497}
}


ðŸ… Author
William Alexander Ousley
PMP â€¢ CSIE â€¢ CSAP
AI/ML Researcher â€” Autonomous Trading Systems
ORCID: https://orcid.org/0009-0009-2503-2010

---

## ðŸ¤ Contributors

MiniCrit is an independent research project maintained by:

- **William Alexander Ousley** â€” Creator, lead researcher, dataset engineer, and model developer.

Community contributions (datasets, rebuttal samples, improvements to the LoRA pipeline, reproducibility fixes, etc.) are welcome.  
Submit pull requests or open issues if you'd like to collaborate.

---

## ðŸ’  Funding & Acknowledgements

This project is part of an ongoing effort to build transparent, open-source adversarial evaluators for financial LLM systems.

Special acknowledgements:

- **Lambda Labs Research Grant (Pending Review)** â€” Application submitted for 2,000 A100-80GB compute hours for ATAC-LoRA scaling.
- **CloudRift Research Grant (Under Review)** â€” Application submitted for 1,000 GPU hours.
- **HuggingFace** â€” Hosting the FinRebut-600 dataset.
- **Zenodo / CERN** â€” Providing DOI-based archival for v1.2.0.
- **GitHub** â€” Repository distribution ecosystem.

This work is completely independent and not affiliated with any institution, employer, or sponsor.

---

## ðŸ§­ Project Roadmap (2025)

**Phase 1 â€” Dataset Expansion (Q4 2025)**
- Expand FinRebut-600 â†’ **FinRebut-2000**
- Add *high-volatility* and *macro-driven* rationale categories
- Introduce multi-rater adjudication (LLM + human)

**Phase 2 â€” Model Improvements**
- Scale MiniCrit-1.5B â†’ **MiniCrit-3B** (LoRA or QLoRA)
- Add multi-LLM adversarial scoring (cross-model consensus)
- Integrate chain-of-thought detection for hallucination traps

**Phase 3 â€” Evaluation Framework**
- Build a standalone **MiniCrit Evaluator API**
- Create benchmark tasks for:
  - fallacy detection  
  - weak reasoning detection  
  - hallucination classification  
  - adversarial rebuttal generation

**Phase 4 â€” Research Publication**
- Draft full technical report (8â€“12 pages)
- Publish on Zenodo / TechRxiv
- Prepare conference-style appendix (datasets, methods, ablations)

---
## ðŸ”„ System Workflow

```mermaid
flowchart TD

A[User or LLM Generates Trading Rationale] --> B[MiniCrit Model]
B --> C{Critique?}
C -->|Weak Reasoning| D[Generate Rebuttal]
C -->|Acceptable| E[Score & Pass Forward]

D --> F[Store in FinRebut Dataset]
F --> G[Nightly ATAC-LoRA Training]

E --> H[Ensemble Validator]
H --> I[Autonomous Trading Engine]

If Mermaid is not rendered by GitHub on mobile, include this ASCII fallback underneath:

[ Rationale ] â†’ [ MiniCrit ] â†’ { Acceptable? }
| Yes â†’ Score â†’ Validator â†’ Trade Engine
| No â†’ Rebuttal â†’ Dataset â†’ Nightly Training


